[
  {
    "title": "Architecture",
    "url": "/ai-proxy/developer-guide\\architecture/",
    "category": "developer-guide",
    "tags": null,
    "content": "layout: default title: Architecture Architecture System architecture overview for GLM Proxy. **Provider Context:** This proxy is designed for Z.ai (GLM Coding Plan). See Z.ai Knowledge Base for model specifications and API details. Overview GLM Proxy is a Node.js-based HTTP proxy that provides intelligent request routing, load balancing, and fault tolerance for API requests to the Z.AI GLM service. Dashboard Architecture The dashboard provides a visual interface to monitor all system components: Key UI Components **Page Navigation** - Switch between Overview, Model Routing, Requests, and System views: **Connection Status** - Real-time upstream connectivity indicator: **Keys Heatmap** - Visual representation of API key health: **Request Rate Chart** - Monitor throughput over time: System Components Core Modules Entry Point - **** - Main server entry point, initializes all components Configuration - **** - Configuration management with environment variable overrides and normalization - **** - Loads model pricing from with change detection Logging - **** - Structured JSON logging Key Management - **** - API key rotation, health scoring, and circuit breaker logic - **** - Intelligent key selection with health-weighted scoring and drift detection (1067 lines) - **** - Per-key circuit breaker implementation Request Handling - **** - Proxy logic with retries, backoff, and streaming - **** - Route registry and dispatch - **** - 15 controller files (health, auth, stats, etc.) Model Routing - **** - Complexity-aware tiered model routing with overrides (3734 lines) - **** - Per-model cost tracking with external pricing configuration (1121 lines) Concurrency Control - **** - AIMD-based per-model concurrency controller (465 lines) - **** - Token bucket rate limiting Metrics - **** - Real-time metrics collection - **** - O(1) latency tracking for memory efficiency (204 lines) - **** - File I/O for stats storage (236 lines) Dashboard - **** - Dashboard HTML/CSS/JS generation Data "
  },
  {
    "title": "Claude Code Setup for AI Proxy",
    "url": "/ai-proxy/developer-guide\\claude-code-setup/",
    "category": "developer-guide",
    "tags": null,
    "content": "layout: default title: Claude Code Setup for AI Proxy Claude Code Setup for AI Proxy This guide helps you use Claude Code (Anthropic's AI CLI) effectively with this project. **Using Z.ai (GLM Coding Plan)?** See Z.ai Knowledge Base for specific setup instructions with GLM models. Project Context AI Proxy is a high-performance API proxy written in Node.js using **CommonJS** (not ESM). Key characteristics: - Single-threaded event loop with optional clustering - Circuit breaker pattern for fault tolerance - AIMD (Additive Increase/Multiplicative Decrease) for adaptive concurrency - Real-time dashboard with Server-Sent Events Recommended Claude Code Instructions Create in your project root: Common Workflows Debugging a Failing Test 1. Read the test file to understand what's failing 2. Check related module imports 3. Use for detailed output 4. Add debug logging if needed Adding a New Feature 1. Write test first (TDD) 2. Run test to confirm it fails 3. Implement feature 4. Run tests to confirm passing 5. Update documentation if needed Refactoring 1. Ensure all tests pass first 2. Make small changes 3. Run tests frequently 4. Keep functionality identical Tips for Best Results - Ask for specific file paths when referring to code - Reference the test file when discussing test failures - Mention the error message when debugging - Ask for the specific test case when tests fail"
  },
  {
    "title": "SSE Event Contract",
    "url": "/ai-proxy/developer-guide\\events/",
    "category": "developer-guide",
    "tags": null,
    "content": "layout: default title: SSE Event Contract SSE Event Contract This document describes the Server-Sent Events (SSE) contract for the (alias: ) endpoint. Dashboard Visualization The Requests page displays real-time events from the SSE stream: **Trace Table** - View individual request events: **Log Entries** - Real-time event log: Connection **Endpoint:** or **Query Parameters:** - (optional): Comma-separated list of event types to subscribe to. Default: - Valid types: , , , **Example:** Event Contract Every SSE message follows this contract: Standard Message Envelope All event payloads include these fields: | Field | Type | Description | |-------|------|-------------| | | number | Monotonically increasing sequence number (global, per-broadcast) | | | number | Unix timestamp in milliseconds when event was created | | | number | Schema version for backward compatibility (currently ) | | | string | Event type name (matches the line) | Plus event-specific fields described below. **Sequence Number Semantics:** - is a global counter incremented once per broadcast event - All clients subscribed to the same event type receive the same value - The event has its own seq (unique to that client's connection) - Clients can detect missed events by checking - Note: If you're subscribed to a subset of event types, you'll see gaps in seq (this is expected) Event Types Sent immediately on connection. Includes recent requests for initial state hydration. | Field | Type | Description | |-------|------|-------------| | | string | Unique identifier for this SSE connection | | | string[] | Event types this client is subscribed to | | | object[] | Last 50 requests for initial state | Emitted for each proxied request completion. **Critical event** - clients that can't keep up may be disconnected. | Field | Type | Description | |-------|------|-------------| | | string | Unique request identifier | | | number | Index of the API key used | | | string | First 8 characters of the key | | | number "
  },
  {
    "title": "Testing",
    "url": "/ai-proxy/developer-guide\\testing/",
    "category": "developer-guide",
    "tags": null,
    "content": "layout: default title: Testing Testing Test strategy and coverage information for GLM Proxy. E2E Testing with Screenshots The project uses Playwright for E2E testing, including automated screenshot generation for documentation: Screenshot Generation Generate screenshots for documentation: See Screenshots Gallery for all available screenshots. Running Tests Run All Tests Run with Coverage Test Structure Tests are located in the directory: Test Strategy Unit Tests - Individual module testing in isolation - Mocked dependencies - Fast execution Integration Tests - End-to-end request flow - Component interaction - Real HTTP server (test port) Coverage Tracking Current coverage status is tracked in: - **** - Coverage gaps and improvement plan Coverage Goals | Module | Target | Current | |--------|--------|---------| | Core logic | 90%+ | TBD | | Error handling | 95%+ | TBD | | Edge cases | 80%+ | TBD | Writing New Tests Test Template Test Quarantine Tests that are temporarily disabled are tracked in: - **** - Quarantined tests and exit criteria Continuous Integration Tests run automatically on: - Every push - Every pull request - Before release Debugging Tests Run Single Test File Run Specific Test Debug Mode Test Data Test fixtures and mocks: - Mock API keys in test setup - Sample responses for testing - Error scenario mocks See Also - TEST_PLAN.md - Comprehensive test strategy - COVERAGE_DEBT.md - Coverage tracking"
  },
  {
    "title": "Chaos Mode: Cross-Model Concurrent Throughput Maximization",
    "url": "/ai-proxy/features\\chaos-mode/",
    "category": "features",
    "tags": null,
    "content": "layout: default title: Chaos Mode: Cross-Model Concurrent Throughput Maximization Chaos Mode: Cross-Model Concurrent Throughput Maximization Dashboard Visualization The Model Routing page shows available models and their configurations: **Model List** - View all available models with their tier classifications: Problem Statement With z.ai's per-model concurrency limits, a single model bottlenecks at its limit (e.g., glm-4.7 at 3 concurrent). When running parallel Claude Code agents (oh-my-claudecode's ultrawork/swarm modes), requests queue behind the same model's rate limit even though other models have spare capacity. **Chaos mode** distributes requests across ALL available GLM models to maximize aggregate concurrent throughput, treating the entire z.ai model catalog as a single pool rather than routing everything to one model per tier. Rate Limit Reference (z.ai Subscription) | Model | Concurrent | Tier | Cost (in/out per 1M) | |-------|-----------|------|---------------------| | glm-4.7 | 3 | HEAVY | $0.60 / $2.20 | | glm-4.6 | 3 | HEAVY | $0.60 / $2.20 | | glm-4.5-x | 2 | HEAVY | $2.20 / $8.90 | | glm-4.5 | 10 | MEDIUM | $0.60 / $2.20 | | glm-4.5-air | 5 | MEDIUM | $0.20 / $1.10 | | glm-4.7-flashx | 3 | LIGHT | $0.07 / $0.40 | | glm-4.7-flash | 1 | LIGHT | FREE | | glm-4.5-flash | 1 | LIGHT | FREE | **Max aggregate concurrency:** 28 simultaneous requests (all models combined) Design Principles 1. **Opt-in** — Chaos mode is a config flag, default off. Existing behavior unchanged. 2. **Quality floor** — Users set a minimum tier. Chaos never routes Opus requests to flash models. 3. **Concurrency-aware** — Prefer models with available capacity, not just round-robin. 4. **Existing infrastructure** — Build on , , and per-model cooldown tracking. 5. **Observable** — Dashboard shows which models are active, per-model utilization, chaos mode status. Architecture Config Shape Model Pool Manager (new class) Selection Strategies 1. (recommended default) Picks the model with"
  },
  {
    "title": "Model Mapping Backend Implementation",
    "url": "/ai-proxy/features\\model-mapping/",
    "category": "features",
    "tags": null,
    "content": "layout: default title: Model Mapping Backend Implementation Model Mapping Backend Implementation Dashboard Visualization The Model Routing page displays model mappings and configurations: **Model List** - View available models with their tier classifications: Summary Successfully implemented the backend for live model management in the GLM Proxy. This includes: 1. **ModelMappingManager class** - Manages model mappings with per-key overrides 2. **API endpoints** - RESTful endpoints for live model mapping management 3. **Request handler integration** - Updated to use the model mapping manager Changes Made 1. lib/config.js Added ModelMappingManager Class (lines 409-536) **Constructor:** - Takes model mapping config object - Initializes with enabled flag, models map, default model, and logging preference - Creates empty Map for per-key overrides **Methods:** - - Returns mapped model name - Checks per-key overrides first (highest priority) - Falls back to global mapping - Uses default model if set - Passes through unchanged if no mapping found - Respects enabled flag - - Sets per-key override - Creates override map for key if doesn't exist - Maps specific Claude model to GLM model for that key only - - Clears per-key override - Clears specific model override if claudeModel provided - Clears all overrides for key if claudeModel omitted - Removes empty key entries - - Updates global configuration - Merges new mapping configuration - Can update enabled, models, defaultModel, logTransformations - - Resets to default configuration - Restores default settings - Clears all per-key overrides - - Exports current configuration - Returns plain object with current settings - - Gets all per-key overrides - Returns Map as plain object - - Gets overrides for specific key - Returns override map or null Updated Config Class **Constructor (line 198):** - Added call to **New Method (lines 342-352):** - - Creates ModelMappingManager instance from config **New Getter (lines 354-357):** - - R"
  },
  {
    "title": "Model Routing",
    "url": "/ai-proxy/features\\model-routing/",
    "category": "features",
    "tags": null,
    "content": "layout: default title: Model Routing Model Routing Complexity-aware routing layer that sits above the legacy . Routes Anthropic requests to GLM target models based on request complexity, supporting rule-based matching, heuristic classification, per-model cooldowns, and failover. When (default), the router is constructed but returns . All traffic falls through to the legacy . You can configure the router while disabled and toggle on without restart. **Related Documentation:** - Z.ai Coding Subscription Reference - Quick tier comparisons and limits - Z.ai Knowledge Base - Comprehensive model specs, API details, and integrations Dashboard Visualization Accessing the Routing Page Navigate to the Model Routing page using the page navigation tabs: Model List View The routing page displays all configured models with their tier assignments, pricing, and concurrency limits: Each model card shows: - Model name and tier assignment - Input/output token pricing - Current and maximum concurrency - Add/remove controls for tier management Tier Builder Configure routing tiers using the drag-and-drop interface: The tier builder lets you: - Assign models to tiers (light, medium, heavy) - Configure fallback chains for each tier - Set routing strategies (balanced, quality, throughput) Quick Start Minimal config to enable routing: This routes all requests through the heuristic classifier. Light requests go to , medium requests to , and heavy requests to . Routing Precedence Strict order, first match wins: 1. **Per-request UI override** -- header (requires admin auth) 2. **Saved overrides** -- set via dashboard or API 3. **Config rules** -- evaluated in array order, first match wins 4. **Heuristic classifier** -- only when at least one tier has 5. ** fallback** 6. **Legacy ** -- when router returns 7. **Passthrough** -- original model unchanged When the router selects a model (steps 1--5), the result bypasses the legacy entirely. Configuration Reference Full config block with defaults: Ti"
  },
  {
    "title": "AI Proxy",
    "url": "/ai-proxy/index/",
    "category": ".",
    "tags": null,
    "content": "AI Proxy - Intelligent AI Request Router AI Proxy Intelligent AI Request Router with Smart Monitoring GitHub Repository Get Started Release Notes Search Documentation (Ctrl+K) &times; Loading search index... Quick Start New to AI Proxy? Start here to get up and running quickly. Getting Started Start Here Installation, setup, and your first request Dashboard Guide Visual tour of the monitoring dashboard with screenshots Configuration Environment variables and api-keys.json options Monitoring Health checks, stats, and backpressure endpoints Features Explore AI Proxy's powerful features for intelligent request routing and management. Model Routing Complexity-aware request routing to optimize costs Model Mapping Model name mapping configuration and aliases Chaos Mode Chaos testing and fault injection capabilities Developer Guide Documentation for contributors and integrators building with AI Proxy. Architecture System architecture and design principles Events Event system documentation and extensions Testing Test strategy, coverage, and CI/CD Claude Code Setup AI-assisted development environment setup Operations Guides for deploying and operating AI Proxy in production. Security Security considerations and best practices Load Testing Performance testing procedures and benchmarks Admin API Cost tracking admin API reference Reference API references, metrics, and comprehensive documentation. Metrics Month 1 metrics and performance data Z.ai Coding Subscription Complete reference for Z.ai GLM tiers and limits Z.ai Knowledge Base Comprehensive Z.ai documentation and history Additional Resources Release Notes Version history and changelog Issue Tracker Report bugs and request features Screenshots Gallery Visual documentation of the UI AI Proxy v2.0.1 | GitHub"
  },
  {
    "title": "Z.AI Model Concurrency & Availability Findings",
    "url": "/ai-proxy/model-concurrency-findings/",
    "category": ".",
    "tags": null,
    "content": "layout: default title: Z.AI Model Concurrency & Availability Findings Z.AI Model Concurrency & Availability Findings **Date:** 2026-02-17/18 **Subscription:** Coding Plan **Base URL:** **Total Keys:** 20 **Related:** See Z.ai Knowledge Base for complete documentation on tiers, pricing, and API configuration. **Quick Reference:** See Z.ai Coding Subscription for tier limits and quotas. Key Finding: Concurrency is PER-ACCOUNT, not per-key Multi-key tests on and both confirm: - Spreading requests across multiple API keys does NOT increase throughput - All 20 keys share the same account-level concurrency quota - **The router's (= number of keys) is wrong and massively inflates capacity estimates** Model Availability Working Models (Coding Subscription) | Model | Status | Observed Max Conc (1 key) | Metadata maxConc | Notes | |-------|--------|--------------------------|-----------------|-------| | Model | Status | Max Clean Conc | First 429 | Consistent 429 | Metadata | |-------|--------|---------------|-----------|----------------|----------| | | OK | **15** | 10 (sporadic) | 19 | 3 | | | OK | **8+** | not seen up to 8 | - | 3 | | | OK | **8+** | not seen up to 8 | - | 10 | | | OK | **15** | 10 (sporadic) | 17 | 5 | | | OK | 1 (not stress-tested) | - | - | 1 | | | OK | 1 (not stress-tested) | - | - | 2 | | | OK | 1 (not stress-tested) | - | - | 1 | BLOCKED Models (Error 1113: \"Insufficient balance or no resource package\") These models return HTTP 429 with Z.AI error code at **any** concurrency level, including 1. They are NOT available on the Coding Plan subscription and should be removed from tier routing. | Model | Error Code | Error Message | |-------|-----------|---------------| | | 1113 | Insufficient balance or no resource package | | | 1113 | Insufficient balance or no resource package | | | 1113 | Insufficient balance or no resource package | | | 1113 | Insufficient balance or no resource package | INVALID Models (Error 1211: \"Unknown Model\") These model IDs do"
  },
  {
    "title": "Cost Tracking Admin API Quick Reference",
    "url": "/ai-proxy/operations\\admin-api/",
    "category": "operations",
    "tags": null,
    "content": "layout: default title: Cost Tracking Admin API Quick Reference Cost Tracking Admin API Quick Reference Dashboard Visualization The Cost Panel in the dashboard provides real-time visibility into cost tracking: Base URL Authentication All endpoints require admin authentication when enabled: Endpoints GET /config Get current cost tracking configuration. **Response:** - - Default pricing rates - - Per-model pricing - - Budget settings - - Save debounce delay - - Persistence file path **Example:** POST /config Update cost tracking configuration. **Request Body (all fields optional):** **Validation:** - Rates must be non-negative numbers - Budget thresholds must be 0-1 **Example:** GET /metrics Get detailed cost tracking metrics. **Response:** **Example:** POST /flush Force immediate save to disk. **Response:** **Use when:** - Before server shutdown - Before backup - To ensure data persistence **Example:** POST /reset Reset all cost tracking data. **⚠️ Warning:** Destructive operation, cannot be undone. **Response:** **Example:** HTTP Status Codes - - Success - - Bad request (validation error) - - Unauthorized (missing/invalid token) - - Method not allowed - - Internal server error - - Service unavailable (cost tracking not enabled) Common Operations Update pricing for a new model Set daily budget with alerts Save data before backup Check system health Error Handling Validation Error (400) Authentication Error (401) Service Unavailable (503) Best Practices 1. **Always authenticate** - Use admin tokens for all requests 2. **Validate changes** - Check config after updates 3. **Flush before shutdown** - Ensure data persistence 4. **Monitor metrics** - Check metrics regularly for issues 5. **Backup before reset** - Always backup before destructive operations 6. **Use thresholds wisely** - Set alerts at appropriate levels Security Notes - All endpoints require admin authentication when enabled - Sensitive operations (flush, reset) require POST method - Input validation prevent"
  },
  {
    "title": "Load Testing Guide",
    "url": "/ai-proxy/operations\\load-testing/",
    "category": "operations",
    "tags": null,
    "content": "layout: default title: Load Testing Guide Load Testing Guide This document describes how to run load tests and validate performance baselines for the GLM Proxy. Dashboard Monitoring During Load Tests The dashboard provides real-time visibility into system performance during load tests. Key sections to monitor: **Error Breakdown** - View error types during load tests: **Retry Analytics** - Monitor retry behavior under load: **Health Score** - Track overall system health: Quick Start Performance Baselines Baselines are defined in . Each profile specifies: - **Duration**: How long to run the test - **RPS**: Target requests per second - **Concurrency**: Maximum concurrent requests - **Warmup**: Seconds to warm up before measuring - **Thresholds**: Pass/fail criteria Available Profiles | Profile | Duration | RPS | Use Case | |---------|----------|-----|----------| | smoke | 30s | 5 | Quick CI validation | | standard | 120s | 20 | Regression detection | | stress | 300s | 50 | Capacity planning | | soak | 1hr | 10 | Memory leak detection | Threshold Categories 1. **Latency** (milliseconds) - P50: Median response time - P95: 95th percentile - P99: 99th percentile 2. **Throughput** - Minimum requests per second 3. **Errors** - Maximum error rate (percentage) 4. **Memory** - Maximum heap usage (MB) - Maximum leak rate (MB/minute) Running Tests Prerequisites 1. Start the proxy server: 2. Ensure you have API keys configured in your or Basic Load Test Baseline Validation Stress Tests with Jest Interpreting Results Success Output Failure Output CI Integration Add baseline validation to your CI pipeline: Updating Baselines When performance improves, update the baseline thresholds: 1. Run a standard baseline test to get current metrics 2. Update with new thresholds 3. Add a buffer (e.g., 20%) above current metrics for variability 4. Document the change with date and reason Example update: Troubleshooting High Latency 1. Check if upstream API is slow 2. Verify connection pooling is "
  },
  {
    "title": "Security Configuration Guide",
    "url": "/ai-proxy/operations\\security/",
    "category": "operations",
    "tags": null,
    "content": "layout: default title: Security Configuration Guide Security Configuration Guide This document describes security settings for the GLM Proxy and provides guidance for safe deployment. Dashboard Security The dashboard provides visibility into system health and security status: **Connection Status** - Verify secure connectivity to upstream API: **Keys Heatmap** - Monitor API key health for security incidents: Security Modes The proxy supports three security modes that configure sensible defaults for different deployment scenarios. Local Mode (Default) **Best for:** Development and trusted internal networks. - Dashboard accessible without authentication - Admin endpoints accessible without authentication - CSP headers disabled - Logging includes full request/response bodies Internet Mode **Best for:** Internet-facing deployments, production systems. Automatically enables: - Admin authentication required - CSP headers with strict policy - Sensitive endpoints restricted - Request/response body logging redacted - Rate limiting on admin endpoints Custom Mode For fine-grained control, set and configure individual settings: Configuration Reference Admin Authentication Pass tokens via the header: Content Security Policy (CSP) CSP prevents XSS attacks by controlling which resources can be loaded. Rate Limiting Protect against abuse with rate limits: Security Checklist Before deploying to production, verify: - [ ] is or with appropriate settings - [ ] Admin tokens are set and kept secret - [ ] API keys are loaded from environment or secure file - [ ] CSP headers are enabled - [ ] Logging does not expose sensitive data - [ ] TLS is enabled (reverse proxy or native) Common Unsafe Configurations The following configurations are flagged as unsafe by the config linter: | Setting | Risk | Recommendation | |---------|------|----------------| | on non-localhost | Dashboard exposed to network | Use mode | | on internet | Anyone can pause/control proxy | Enable admin auth | | on internet"
  },
  {
    "title": "AI Proxy Documentation",
    "url": "/ai-proxy/README/",
    "category": ".",
    "tags": null,
    "content": "layout: default title: Documentation AI Proxy Documentation Welcome to the AI Proxy documentation. This site is built with Jekyll and hosted on GitHub Pages. Local Development To test the documentation site locally: Prerequisites - Ruby (version 2.5 or higher) - Bundler: Build the Site 1. Install dependencies: 2. Build the site: 3. Serve locally (optional): Then visit Quick Test Run the test script: Adding New Documentation 1. Create a new markdown file in the appropriate directory under 2. Add front matter at the top: 3. Add a link to it in (without the extension) Link Format - **External links in index.html**: Use directory format with trailing slash - Correct: - Incorrect: - **Internal markdown links**: Use directory format with trailing slash - Correct: - Incorrect: Architecture - ****: Main layout template with styling - ****: Jekyll configuration - ****: Landing page with navigation - ****: Documentation content with front matter Styling The documentation uses a custom gradient design with: - Purple gradient background (#667eea to #764ba2) - White content cards - Responsive grid layout - Syntax highlighting for code blocks Deployment The site is automatically deployed to GitHub Pages when changes are pushed to the main branch. The site is available at: https://richertunes.github.io/ai-proxy/"
  },
  {
    "title": "Month 1 Metrics: 429 Efficiency & Retry Intelligence",
    "url": "/ai-proxy/reference\\metrics/",
    "category": "reference",
    "tags": null,
    "content": "layout: default title: Month 1 Metrics: 429 Efficiency & Retry Intelligence Month 1 Metrics: 429 Efficiency & Retry Intelligence Metrics added to quantify retry waste, give-up behavior, model routing effectiveness, and tier downgrade impact under 429 pressure. All counters are monotonic (Prometheus type) and reset only on proxy restart or explicit . **Related:** See Model Routing for configuration details and Z.ai Knowledge Base for provider-specific rate limit behavior. Endpoints | Endpoint | Format | Includes Month 1? | |----------|--------|-------------------| | | JSON | Yes (, , top-level keys) | | | Prometheus exposition | Yes (all counters below) | Metric Families 1. Give-Up Tracking Fires when the retry loop stops early instead of exhausting all attempts. | Prometheus Metric | Type | Description | |-------------------|------|-------------| | | counter | Total early give-up events | | | counter | Give-ups from hitting max consecutive 429 attempts | | | counter | Give-ups from exceeding the 429 time window | **JSON path:** , , **Recording sites:** — two paths inside the LLM 429 retry acceptance logic (one for attempt cap, one for window cap). 2. Retry Efficiency Quantifies how well the pool and model router serve alternative models during retries. | Prometheus Metric | Type | Description | |-------------------|------|-------------| | | counter | Retries where pool assigned the same model already tried (waste) | | | counter | Cumulative count of distinct models tried across all failed requests | | | counter | Cumulative model switches across all failed requests | | | counter | Failed requests that had model routing (denominator for averaging) | **JSON path:** , , , **Recording sites:** - : Inside 429 acceptance path only. A flag is set in the model tracking block (before ), then checked inside the LLM 429 acceptance path (after ). This ensures the counter fires only for 429-driven retries, not on breaks, cap-reached breaks, or non-429 errors. - : Fires exactly o"
  },
  {
    "title": "Z.ai GLM Coding Plan - Complete Reference",
    "url": "/ai-proxy/reference\\zai-coding-subscription/",
    "category": "reference",
    "tags": null,
    "content": "layout: default title: Z.ai GLM Coding Plan - Complete Reference Z.ai GLM Coding Plan - Complete Reference **Last Updated:** February 21, 2026 **Provider:** Zhipu AI (Z.ai / BigModel.cn) **Documentation Sources:** z.ai | docs.z.ai | open.bigmodel.cn **Looking for more details?** See the Z.ai Knowledge Base for comprehensive documentation including model specs, integrations, known issues, and historical changes. Quick Summary Z.ai's GLM Coding Plan is a subscription-based AI coding service marketed as **\"1/7 the cost of Claude, 3x the usage\"**. It uses Zhipu AI's GLM models (GLM-4.7, GLM-5) and is compatible with major coding tools like Claude Code, Cursor, Cline, and Roo Code. | Feature | Value | |---------|-------| | **Models** | GLM-4.7 (358B params), GLM-5 (754B params) | | **Context Window** | 128K - 200K tokens | | **Pricing Range** | $10 - $80/month (International) | | **Quota System** | 5-hour rolling window (prompts) | | **API Compatibility** | OpenAI + Anthropic compatible | Table of Contents 1. Subscription Tiers & Pricing 2. Usage Limits & Quotas 3. Model Access by Tier 4. API Endpoints & Authentication 5. Concurrency & Rate Limits 6. MCP Tool Limits 7. Integration with Coding Tools 8. Recent Changes (2026) 9. Comparison with Competitors 10. Troubleshooting & Known Issues Subscription Tiers & Pricing International Pricing (USD) - Post February 12, 2026 | Plan | Monthly | Quarterly | Yearly | |------|---------|-----------|--------| | **Lite** | $10/month | $27/quarter | $84/year | | **Pro** | $30/month | $81/quarter | $252/year | | **Max** | $80/month | $216/quarter | $672/year | China Pricing (CNY) - Post February 12, 2026 | Plan | Monthly | Quarterly | Yearly | |------|---------|-----------|--------| | **Lite** | ¥49/month | ¥132/quarter | ¥411/year | | **Pro** | ¥149/month | ¥402/quarter | ¥1,251/year | | **Max** | ¥469/month | ¥1,266/quarter | ¥3,939/year | Price Increase Notice **Effective February 12, 2026:** Z.ai increased prices by **30%+** and can"
  },
  {
    "title": "Z.ai (Zhipu AI) - Complete Knowledge Base",
    "url": "/ai-proxy/reference\\zai-knowledge-base/",
    "category": "reference",
    "tags": null,
    "content": "layout: default title: Z.ai (Zhipu AI) - Complete Knowledge Base Z.ai (Zhipu AI) - Complete Knowledge Base **Last Updated:** February 21, 2026 @ 16:30 EST **Provider:** Zhipu AI (Z.ai / BigModel.cn) **Purpose:** Comprehensive brain dump of all Z.ai knowledge for integration and reference **See Also:** Z.ai Coding Subscription Quick Reference Table of Contents 1. Executive Summary 2. Company Overview 3. Product Ecosystem 4. Subscription Tiers Deep Dive 5. Model Specifications 6. API Reference 7. Rate Limits & Quotas 8. Integrations 9. Competitive Analysis 10. Known Issues & Workarounds 11. Historical Changes 12. Community Tools 13. Cross-References Executive Summary What is Z.ai? Z.ai (Zhipu AI / 智谱AI) is a Chinese AI company offering GLM (General Language Model) series models through a subscription-based coding plan. Their value proposition is **\"1/7 the cost of Claude, 3x the usage\"**. Key Metrics | Metric | Value | |--------|-------| | **Founded** | 2019 | | **Paid Users (Dec 2025)** | 2.7 million | | **Models** | GLM-4.x series, GLM-5 (754B params) | | **Context Window** | Up to 200K tokens | | **Price Range** | $10-$80/month international | | **SWE Score** | 0.68 (GLM-4.7) | Why Z.ai Matters - **Cost Effective**: Significantly cheaper than Anthropic Claude for coding tasks - **High Quota**: 5-hour rolling window with generous prompt allowances - **Compatible**: Works with Claude Code, Cursor, Cline, Roo Code, and more - **OpenAI/Anthropic Compatible**: Easy drop-in replacement - **Growing Rapidly**: 2.7M paid users as of December 2025 Company Overview Zhipu AI (智谱AI) | Attribute | Detail | |-----------|--------| | **Chinese Name** | 北京智谱华章科技有限公司 | | **Founded** | 2019 | | **Origin** | Tsinghua University research spinoff | | **Headquarters** | Beijing, China | | **Platforms** | z.ai (International), open.bigmodel.cn (China) | | **Developer Community** | GitHub: zai-org | Platform URLs | Purpose | URL | |---------|-----| | **International Dashboard** | | | **Chin"
  },
  {
    "title": "Release Process",
    "url": "/ai-proxy/releases/",
    "category": ".",
    "tags": null,
    "content": "layout: default title: Release Process Release Process This document describes the automated and manual release processes for ai-proxy. Automated Releases Releases are automated using semantic-release. How It Works 1. Push commits to branch 2. CI runs tests and validates 3. semantic-release analyzes commit messages 4. If release needed: creates tag, updates CHANGELOG, publishes to npm Commit Convention Releases follow Conventional Commits: | Type | Bump | Description | |------|------|-------------| | | minor | New feature | | | patch | Bug fix | | | patch | Performance improvement | | | patch | Code refactoring | | | none | Documentation only | | | none | Test only | | | none | Maintenance | | | none | CI changes | Examples: - - - Breaking Changes Add after type and in body: Manual Releases For situations where automated releases don't fit: Release Checklist Before a release: - [ ] All tests passing - [ ] Coverage thresholds met - [ ] CHANGELOG.md updated - [ ] Version in sync After release: - [ ] Verify GitHub release created - [ ] Verify npm package published - [ ] Update announcements"
  },
  {
    "title": "Configuration",
    "url": "/ai-proxy/user-guide\\configuration/",
    "category": "user-guide",
    "tags": null,
    "content": "layout: default title: Configuration Configuration Complete reference for AI Proxy configuration options. **New to configuration?** Start with the Getting Started guide for basic setup. You only need to read this if you want to customize advanced settings. What Are Environment Variables? **Environment variables** are settings that control how the proxy behaves. Think of them like configuration knobs you can adjust without changing any code. **How to set them:** **Mac/Linux:** **Windows PowerShell:** **Windows Command Prompt:** **Quick Reference:** For the most commonly used environment variables, see the root README.md. Environment Variables | Variable | Default | Description | What This Means | |----------|---------|-------------|-----------------| | | | Proxy listen port | Which \"door\" your app uses to connect. Default is fine for most users | | | | Proxy listen address | Which network address to listen on. = only your computer can connect | | | | Target API host | The upstream API service you're connecting to | | | | Maximum cluster workers | How many worker processes to run. More workers = handles more requests, but uses more memory | | | | Set to to disable clustering | Set to if you want to run a single process instead of multiple workers | | | | Maximum retry attempts | How many times to retry a failed request before giving up | | | | Failures before circuit opens | After how many failures a key is temporarily disabled (see \"Circuit Breaker\" below) | | | | Failure window (ms) | Time period (in milliseconds) in which failures are counted | | | | Circuit cooldown period (ms) | How long (in milliseconds) to wait before trying a failed key again | | | | Max concurrent requests per key | Maximum number of simultaneous requests per API key | | | | Max total concurrent requests | Maximum number of simultaneous requests across all keys | | | | Max requests to queue when keys busy | How many requests to wait in line when all keys are busy | | | | Max queue wait time ("
  },
  {
    "title": "Dashboard Guide",
    "url": "/ai-proxy/user-guide\\dashboard/",
    "category": "user-guide",
    "tags": null,
    "content": "layout: default title: Dashboard Guide Dashboard Guide The AI Proxy includes a comprehensive real-time dashboard for monitoring requests, managing routing, and diagnosing issues. Accessing the Dashboard Once the proxy is running, access the dashboard at: Dashboard Navigation Page Navigation Tabs Switch between main dashboard pages using the navigation tabs in the header: | Tab | Description | |-----|-------------| | **Overview** | Key metrics, charts, and live stream | | **Requests** | Live traces, logs, queue, and circuit status | | **Routing** | Model routing configuration and tier management | | **System** | Diagnostics, error breakdown, and health score | Connection Status The header shows the live connection status to the upstream API: - **Green dot** - Connected and receiving events - **Yellow dot** - Connected but no recent events - **Red dot** - Disconnected or connection error Pause/Resume Control Pause the proxy to stop accepting new requests (useful for maintenance): When paused, all incoming requests return until resumed. Dashboard Sections Health Ribbon The top ribbon shows key health metrics at a glance: - **Uptime** - How long the proxy has been running - **Success Rate** - Percentage of successful requests - **Requests/min** - Current request rate - **Active Connections** - Currently processing requests Keys Heatmap Visual representation of API key health and performance: - **Green cells** - Healthy keys with high success rates - **Yellow cells** - Keys with warnings or degraded performance - **Red cells** - Failing or circuit-broken keys - **Color intensity** - Indicates request volume (brighter = more requests) - **Pulsing border** - Key has in-flight requests Cost Panel Track your API spending in real-time: - Current session cost - Projected daily/monthly costs - Per-model cost breakdown Request Charts Real-time charts showing request patterns: - **Request Rate** - Requests per minute over time - **Latency** - Response time distribution (P50, P95,"
  },
  {
    "title": "Getting Started",
    "url": "/ai-proxy/user-guide\\getting-started/",
    "category": "user-guide",
    "tags": null,
    "content": "layout: default title: Getting Started Getting Started A step-by-step guide to get AI Proxy running. **New to this?** Start with the README.md for a quick overview, then come back here for detailed instructions. What You'll Need Before starting, make sure you have: | Requirement | How to Check | Where to Get It | |-------------|--------------|-----------------| | Node.js 18+ | Run | nodejs.org | | A code/text editor | Any editor works | VS Code, Notepad++, etc. | | API keys | N/A | Z.AI Dashboard | | Terminal access | N/A | Terminal (Mac/Linux), Command Prompt or PowerShell (Windows) | Step-by-Step Setup Step 1: Get the Code **Option A: Using Git (recommended)** **Option B: Download ZIP** 1. Go to GitHub 2. Click \"Code\" → \"Download ZIP\" 3. Extract the ZIP file 4. Open terminal in the extracted folder Step 2: Install Dependencies This downloads everything the proxy needs. It may take 1-2 minutes. **What if this fails?** - Make sure Node.js is installed: - Try deleting folder and running again - Check your internet connection Step 3: Get Your API Keys 1. Log in to your Z.AI dashboard 2. Go to \"API Keys\" section 3. Create one or more API keys 4. Copy each key (they look like: ) **Tip:** Having multiple keys lets the proxy spread requests across them, giving you higher rate limits. **Documentation:** - Z.ai Coding Subscription Reference - Quick tier comparisons and limits - Z.ai Knowledge Base - Comprehensive model specs, integrations, and troubleshooting Step 4: Create Your Configuration File Create a file named in the project folder: **For API keys configuration format and advanced options**, see Configuration Guide - api-keys.json Format. **Common mistakes:** - ❌ Missing comma between keys - ❌ Extra comma after the last key - ❌ Using the wrong quotes (must be not ) - ❌ Forgetting to include the Step 5: Start the Proxy **You should see:** **What is ?** This is the address where your proxy is running: - = \"localhost\" (means \"your own computer\") - = the port number (lik"
  },
  {
    "title": "Monitoring",
    "url": "/ai-proxy/user-guide\\monitoring/",
    "category": "user-guide",
    "tags": null,
    "content": "layout: default title: Monitoring Monitoring Guide for monitoring GLM Proxy health and performance. **New to Z.ai?** See Z.ai Documentation for understanding quotas, rate limits, and tier capabilities. Dashboard Monitoring The dashboard provides real-time visual monitoring of all proxy metrics. Open it at . Key Monitoring Sections **Connection Status** - See if the proxy is connected to the upstream API: **Keys Heatmap** - Monitor the health of all your API keys at a glance: - Green cells = healthy keys - Yellow/Red cells = degraded or failing keys - Pulsing cells = active requests **Request Charts** - Track request rate and latency over time: **Circuit Breaker Status** - Monitor circuit breaker states for all keys: **Health Ribbon** - Quick overview of system health at the top of the dashboard: **Cost Panel** - Track token usage and costs in real-time: API Endpoints | Endpoint | Description | |----------|-------------| | | Health check with key status | | | Real-time statistics | | | Historical usage statistics | | | Current load information | | | Trigger hot reload of API keys | Health Check Check proxy health and key status: **Response:** Fields | Field | Description | |-------|-------------| | | Overall health status ( or ) | | | Number of healthy API keys | | | Total number of configured keys | | | Uptime in seconds | | | Current backpressure information | Real-time Statistics View detailed real-time statistics: **Response:** Key Metrics | Metric | Description | |--------|-------------| | | Uptime in seconds | | | Total requests processed | | | Current requests per minute | | | Overall success rate percentage | | | Currently active connections | Latency Metrics | Field | Description | |-------|-------------| | | Average latency in milliseconds | | | Minimum latency | | | Maximum latency | | | 50th percentile (median) | | | 95th percentile | | | 99th percentile | Error Metrics | Field | Description | |-------|-------------| | | Number of timeout errors | | | Soc"
  }
]